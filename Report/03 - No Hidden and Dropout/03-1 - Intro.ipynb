{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "announced-excitement",
   "metadata": {},
   "source": [
    "# Reading this section\n",
    "\n",
    "Read in numerical order. The appendix contains additional data used in the assessment such as shell scripts and other useful files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-stream",
   "metadata": {},
   "source": [
    "# Preparing our data in Python for use in Neural Networks.\n",
    "\n",
    "For use in Neural Networks we first need to pre-process our data into a suitable format for use with the Tensorflow package in order to compile and fit a Neural Network on our data that classifies attacks.\n",
    "\n",
    "Throughout the report you will find similar methods used for each different network type that we attempted to implement, in order to achieve more consistent results, and these data processing techniques were shared amongst the group. The most important step is when we split the data for training and testing, where those of us using python made sure to specify the same `random_state` variable in order to produce the same data splitting.\n",
    "\n",
    "We also each recreate these steps as we would send most of this processing off in our `.py` File that we submit to the HPC in order to avoid package discrepancy issues. At first I tried to save my training and testing data as pickle files, before copying them to the HPC and then accessing them with a Python script. Unfortunately due to discrepancies in the versions of Python packages this proved to be more trouble than it was worth and a lot of this data processing was then ported to the final Python script I submitted to the HPC.\n",
    "\n",
    "We start by importing important packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import squarify\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import gzip\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-dylan",
   "metadata": {},
   "source": [
    "Now we import our data from the repository, saved in ZIP format in the GitHub respository to save space. These are split in 4 as the original data was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.datetime.now()\n",
    "print(\"Reading df1\")\n",
    "df1 = pd.read_csv(\"https://github.com/Galeforse/DST-Assessment-04/raw/main/Data/UNS1.zip\",header=None)\n",
    "print(\"Reading df2\")\n",
    "df2 = pd.read_csv(\"https://github.com/Galeforse/DST-Assessment-04/raw/main/Data/UNS2.zip\",header=None)\n",
    "print(\"Reading df3\")\n",
    "df3 = pd.read_csv(\"https://github.com/Galeforse/DST-Assessment-04/raw/main/Data/UNS3.zip\",header=None)\n",
    "print(\"Reading df4\")\n",
    "df4 = pd.read_csv(\"https://github.com/Galeforse/DST-Assessment-04/raw/main/Data/UNS4.zip\",header=None)\n",
    "print(\"Data fetched in:\" ,dt.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-saint",
   "metadata": {},
   "source": [
    "We now concatenate the data frames (ignoring index in order to avoide any issues that may arise with duplicate indexes) and then add column names to the data. These columns names are adapted from the information provided alongside the data set on it's website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2,df3,df4],ignore_index=True)\n",
    "df.columns = ['source_ip', 'source_port', 'dest_ip', 'dest_port', 'proto', 'state', 'duration', 'source_bytes', 'dest_bytes', 'source_ttl',\n",
    "             'dest_ttl', 'source_loss', 'dest_loss', 'service', 'source_load', 'dest_load', 'source_pkts', 'dest_pkts', 'source_TP_win', 'dest_TP_win', \n",
    "             'source_tcp_bn', 'dest_tcp_bn', 'source_mean_sz', 'dest_mean_sz', 'trans_depth', 'res_bdy_len', 'source_jitter', 'dest_jitter', 'start_time',\n",
    "             'last_time', 'source_int_pk_time', 'dest_int_pk_time', 'tcp_rtt', 'synack', 'ackdat', 'is_sm_ips_ports', 'count_state_ttl', \n",
    "             'count_flw_http_mthd', 'is_ftp_login', 'count_ftp_cmd', 'count_srv_source', 'count_srv_dest', 'count_dest_ltm',\n",
    "             'count_source_ltm', 'count_source_destport_ltm', 'count_dest_sourceport_ltm', 'counts_dest_source_ltm', 'attack_cat', 'Label']\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-coupon",
   "metadata": {},
   "source": [
    "Next we are going to check some of the features in our data. We check the size of the data frame and then take horizontal slices of the data as it is impossible to view all 49 columns in one view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.iloc[:,0:16]\n",
    "df_2 = df.iloc[:,16:32]\n",
    "df_3 = df.iloc[:,32:49]\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-bicycle",
   "metadata": {},
   "source": [
    "We briefly check that our slices are the size we expect them to be (same number of rows as main data frame, and columns add up to 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of 1st slice:\")\n",
    "print(df_1.shape)\n",
    "print(\"shape of 2nd slice:\")\n",
    "print(df_2.shape)\n",
    "print(\"shape of 3rd slice:\")\n",
    "print(df_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-aaron",
   "metadata": {},
   "source": [
    "We will describe each slice to check for any anomalous data values such as `NaN` and `inf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-robert",
   "metadata": {},
   "source": [
    "We want to check that our data is suitable for use on different types of machine, as most PCs are 64 bit, however the HPC runs a 32 bit system and we don't want there to be any errors arising from this bit difference. \n",
    "\n",
    "We start by defining the maximum value of each variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmax = df.max()\n",
    "dfmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-thursday",
   "metadata": {},
   "source": [
    "We define the following function to iterate through a data set and check that it passes the bit limit for 64 bit first (more likely to pass than 32 bit) then 32 bit. We then run the function on our `dfmax` as these are the maximums of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitcheckmax(data):\n",
    "    for i in data:\n",
    "        count = 0\n",
    "        fail = False\n",
    "        if isinstance(i,str) == True:\n",
    "            pass\n",
    "        else:\n",
    "            j = float(i)\n",
    "            count = count+1\n",
    "            if j <= np.finfo(np.float64).max:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Fails 64 bit check at row: \" + str(count))\n",
    "                fail = True\n",
    "                break\n",
    "    if fail == False:\n",
    "        print(\"Passes 64 bit check.\")\n",
    "    for i in data:\n",
    "        count = 0\n",
    "        fail = False\n",
    "        if isinstance(i,str) == True:\n",
    "            pass\n",
    "        else:\n",
    "            j = float(i)\n",
    "            count = count+1\n",
    "            if j <= np.finfo(np.float32).max:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"Fails 32 bit check at row: \" + str(count))\n",
    "                fail = True\n",
    "                break\n",
    "    if fail == False:\n",
    "        print(\"Passes 32 bit check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcheckmax(dfmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-journalism",
   "metadata": {},
   "source": [
    "We are interested in classifying attack types. We notice from our earlier look at the data that the `attack_cat` column for normal traffic is NaN (as it obviously isn't an attack, it has no attack category!). We therefore fill in these missing values with the label \"Normal\", to designate normal traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_cat'] = df['attack_cat'].fillna('Normal')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "attackcount = pd.DataFrame(df['attack_cat'].value_counts())\n",
    "attackcount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-anchor",
   "metadata": {},
   "source": [
    "Above we are checking how many of each attack are present in the data; we notice here a problem with the strings used to represent certain attack but we'll address this later in the processing. For now we will make a visual plot of the representation of each attack type in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = []\n",
    "for i in attackcount.index:\n",
    "    ac.append(i)\n",
    "an = attackcount[\"attack_cat\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "ax = fig.add_subplot()\n",
    "fig.set_size_inches(18,8)\n",
    "squarify.plot(label=ac,sizes=an, color = [\"cyan\",\"magenta\",\"yellow\",\"lime\"])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-constitutional",
   "metadata": {},
   "source": [
    "We will now deal with missing data in our data. We will check which columns have missing values and then deal with each accordingly by filling NA values with a 0 as this seems to be the easiest course of action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "colnames = df.columns\n",
    "\n",
    "for name in colnames:\n",
    "    if df[name].isnull().values.any():\n",
    "        l.append(name)\n",
    "        \n",
    "print('The columns with missing values in them are: ' + str(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-lightning",
   "metadata": {},
   "source": [
    "In addition to the above thanks to analysis by others in the group we noticed that the `count_ftp_cmd` column is also missing data but not as NaN values, instead these missing values are blank \" \" single spaces so we'll include this column in our processing even though to python it doesn't look like it has any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percentage of missing data in count_flw_http_mthd column:\" + str(df[\"count_flw_http_mthd\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in is_ftp_login column:\" + str(df[\"is_ftp_login\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in count_ftp_cmd column:\" + str(df[\"count_ftp_cmd\"].isnull().sum()*100/len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "print(\"percentage of missing data in count_flw_http_mthd column:\" + str(df[\"count_flw_http_mthd\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in is_ftp_login column:\" + str(df[\"is_ftp_login\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in count_ftp_cmd column:\" + str(df[\"count_ftp_cmd\"].isnull().sum()*100/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-nothing",
   "metadata": {},
   "source": [
    "As `count_ftp_cmd` is numeric data, we apply the panda function `pd.to_numeric` with the `errors=\"coerce\"` parameter which will coerce the blank spaces (which count as strings) into NaN values, which will show when we use the same functions we've been using again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_ftp_cmd'] = df['count_ftp_cmd'].apply(pd.to_numeric,errors=\"coerce\")\n",
    "print(\"percentage of missing data in count_flw_http_mthd column:\" + str(df[\"count_flw_http_mthd\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in is_ftp_login column:\" + str(df[\"is_ftp_login\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in count_ftp_cmd column:\" + str(df[\"count_ftp_cmd\"].isnull().sum()*100/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-muscle",
   "metadata": {},
   "source": [
    "We fill the NA's with 0 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "print(\"percentage of missing data in count_flw_http_mthd column:\" + str(df[\"count_flw_http_mthd\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in is_ftp_login column:\" + str(df[\"is_ftp_login\"].isnull().sum()*100/len(df)))\n",
    "print(\"percentage of missing data in count_ftp_cmd column:\" + str(df[\"count_ftp_cmd\"].isnull().sum()*100/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-fifth",
   "metadata": {},
   "source": [
    "Now we will deal with the problems we noticed earlier in that some of the attack categories were duplicated due to the structure of their string. Looking back at our earlier findings we write the following block of code to fix this problem. And will see we no longer have any duplicates, and we can repeat our visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['attack_cat'] = df['attack_cat'].map({'Normal': 'Normal', 'Exploits': 'Exploits', ' Fuzzers ': 'Fuzzers', 'DoS': 'DoS',\n",
    "                                          ' Reconnaissance ': 'Reconnaissance', ' Fuzzers': 'Fuzzers', 'Analysis': 'Analysis',\n",
    "                                         'Backdoor': 'Backdoor', 'Reconnaissance': 'Reconnaissance',  ' Shellcode ': 'Shellcode',\n",
    "                                         'Backdoors': 'Backdoor', 'Shellcode': 'Shellcode',  'Worms': 'Worms', 'Generic': 'Generic'})\n",
    "df.groupby('attack_cat').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "attackcount = pd.DataFrame(df['attack_cat'].value_counts())\n",
    "ac = []\n",
    "for i in attackcount.index:\n",
    "    ac.append(i)\n",
    "an = attackcount[\"attack_cat\"].tolist()\n",
    "fig = plt.gcf()\n",
    "ax = fig.add_subplot()\n",
    "fig.set_size_inches(18,8)\n",
    "squarify.plot(label=ac,sizes=an, color = [\"cyan\",\"magenta\",\"yellow\",\"lime\"])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-handy",
   "metadata": {},
   "source": [
    "For use in neural networks prediction we want our data to be numeric however when looking at our data types in the next block we see there is all kinds of different types of data present. We will use a dictionary mapping in order to convert this data into something that is more appropriate. (We also drop the Label column here as it is not useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Label',axis=1)\n",
    "df_source_ip = pd.DataFrame(df['source_ip'])\n",
    "df_source_port = pd.DataFrame(df['source_port'])\n",
    "df_dest_ip = pd.DataFrame(df['dest_ip'])\n",
    "df_dest_port = pd.DataFrame(df['dest_port'])\n",
    "df_proto = pd.DataFrame(df['proto'])\n",
    "df_state = pd.DataFrame(df['state'])\n",
    "df_service = pd.DataFrame(df['service'])\n",
    "df_count_ftp_cmd = pd.DataFrame(df['count_ftp_cmd'])\n",
    "df_attack_cat = pd.DataFrame(df['attack_cat'])\n",
    "\n",
    "# we now create dictionaries to allow us to map onto the data frame\n",
    "\n",
    "sips = df.source_ip.unique()\n",
    "sip_dict = dict(zip(sips,range(len(sips))))\n",
    "\n",
    "sp = df.source_port.unique()\n",
    "sp_dict = dict(zip(sp,range(len(sp))))\n",
    "               \n",
    "dips = df.dest_ip.unique()\n",
    "dip_dict = dict(zip(dips,range(len(dips))))\n",
    "\n",
    "dp = df.dest_port.unique()\n",
    "dp_dict = dict(zip(dp,range(len(dp))))\n",
    "\n",
    "p = df.proto.unique()\n",
    "p_dict = dict(zip(p,range(len(p))))\n",
    "\n",
    "states = df.state.unique()\n",
    "state_dict = dict(zip(states,range(len(states))))\n",
    "\n",
    "services = df.service.unique()\n",
    "service_dict = dict(zip(services,range(len(services))))\n",
    "\n",
    "cfc = df.count_ftp_cmd.unique()\n",
    "cfc_dict = dict(zip(cfc,range(len(cfc))))\n",
    "\n",
    "ac = df.attack_cat.unique()\n",
    "ac_dict = dict(zip(ac,range(len(ac))))\n",
    "\n",
    "df['source_ip_int'] = df['source_ip'].map(sip_dict)\n",
    "df['source_port_int'] = df['source_port'].map(sp_dict)\n",
    "df['dest_ip_int'] = df['dest_ip'].map(dip_dict)\n",
    "df['dest_port_int'] = df['dest_port'].map(dp_dict)\n",
    "df['proto_int'] = df['proto'].map(p_dict)\n",
    "df['state_int'] = df['state'].map(state_dict)\n",
    "df['service_int'] = df['service'].map(service_dict)\n",
    "df['count_ftp_cmd_int'] = df['count_ftp_cmd'].map(cfc_dict)\n",
    "df['attack_cat_int'] = df['attack_cat'].map(ac_dict)\n",
    "\n",
    "df = df.drop('source_ip',axis=1)\n",
    "df = df.drop('source_port',axis=1)\n",
    "df = df.drop('dest_ip',axis=1)\n",
    "df = df.drop('dest_port',axis=1)\n",
    "df = df.drop('proto',axis=1)\n",
    "df = df.drop('state',axis=1)\n",
    "df = df.drop('service',axis=1)\n",
    "df = df.drop('count_ftp_cmd',axis=1)\n",
    "df = df.drop('attack_cat',axis=1)\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-group",
   "metadata": {},
   "source": [
    "Just in case we'll once again check for missingess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "colnames = df.columns\n",
    "\n",
    "for name in colnames:\n",
    "    if df[name].isnull().values.any():\n",
    "        l.append(name)\n",
    "        \n",
    "print('The columns with na/nan values in them are: ' + str(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-andrews",
   "metadata": {},
   "source": [
    "It is good practice to scale our data so that certain features do not heavily weight the learning process. We will also seperate out our `attack_cat` column as this will be what we are going to predict with our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess(data,scaling=None):\n",
    "    data = data.astype(np.float)\n",
    "    if(scaling == None):\n",
    "        scaling = StandardScaler()\n",
    "        datat=scaling.fit_transform(data)\n",
    "    else:\n",
    "        datat=scaling.transform(data)\n",
    "    return(datat,scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['attack_cat_int']\n",
    "X = df.drop('attack_cat_int',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled, scaling = preprocess(X.values)\n",
    "print(X.shape)\n",
    "print(X_scaled.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-crown",
   "metadata": {},
   "source": [
    "We now split our data into testing and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size = 0.1, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-ukraine",
   "metadata": {},
   "source": [
    "The next 2 documents are the python scripts that I submitted to the HPC, they repeats a lot of what has happened in this document and then proceed to define Neural Networks model for classification. I will address these models in detail in the document after but the basics are that each one has different layer configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-invitation",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[So much use of the SLURM documentation in order to understand the available function on the HPC.](https://slurm.schedmd.com/documentation.html)\n",
    "\n",
    "[DST HPC documentation was a good start for using the HPC.](https://dsbristol.github.io/dst/coursebook/appendix5-bluecrystal.html)\n",
    "\n",
    "[BlueCrystal Phase 4 Documentation.](https://www.acrc.bris.ac.uk/protected/bc4-docs/index.html)\n",
    "\n",
    "[Tensorflow tutorial for classification.](https://www.tensorflow.org/tutorials/structured_data/feature_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
