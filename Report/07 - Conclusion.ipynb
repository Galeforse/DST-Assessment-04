{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Our team agreed on the task of classifying types of attacks on a toy dataset which can be found extracted [here](https://github.com/Galeforse/DST-Assessment-04/tree/main/Data) by means of neural networks modelling. Every contributor was free to use any and all variations of NNs on the same inference goal, as well as using other models in parallel for comparison. Our work was performed across 2 main languages: Python and R; as well as the computationally/memory demanding models being run on the University's HPC (High Performance Computer). Each contributor chose to test out at least two main types of models:\n",
    "\n",
    "[Alex](https://github.com/Galeforse/DST-Assessment-04/tree/main/Alex%20Caian) chose to test a _Feed Forward Neural Network_ (FFNN) and a _Single Layer Perceptron_ (SLP)\n",
    "\n",
    "[Gabriel](https://github.com/Galeforse/DST-Assessment-04/tree/main/Gabriel%20Grant) chose to test a _Dropout Neural Network_ and a _No Hidden Layers_ (NHL)/Only Skip Layers NN\n",
    "\n",
    "[Luke](https://github.com/Galeforse/DST-Assessment-04/tree/main/Luke%20Hawley) chose to test an _AutoEncoder Neural Network_ (AE) against a _Naive Bayes_ on the same inference goal and\n",
    "\n",
    "[Matt](https://github.com/Galeforse/DST-Assessment-04/tree/main/Matt%20Corrie) chose to test a _Feed Forward Neural Network_ (FFNN) against an _Extreme Learning Machine_ (ELM) on the same inference goal, as well as _Random Forests_ (RF).\n",
    "\n",
    "The main takeaways were as follows:\n",
    "\n",
    "- **FFNNs** tend to perform really well on the given inference goal, especially so on simpler (particularly uniform) architectures, regardless of the activation function/algorithm. Our team narrowed this aspect down mostly to the fact that the set goal is easily attainable for models in general, as comparisons will tell, so NNs did **not** require much learning. Computation was expectedly long, with even further delays and impediments from the HPC used, and results were both accurate and consistent among themselves.\n",
    "\n",
    "- **The SLP** was only tested on a binary application of attack v.s no attack. Its performance dropped significantly on an increased amount of parameters, suggesting that some features added proportionally more noise than signal to the set target. The accuracy peaked on a 'medium' complexity (i.e. amount of nodes within layer), where it performed slightly worse than all other models. Computation was extremely quick, however - this being its greatest advantage as a method.\n",
    "\n",
    "- **Dropout NNs** achieved high accuracy, comparable to that of FFNNs and NHLs. Uniquely, DNNs presented a stable learning rate all across the training phase and reached convergence quicker than any other implemented model. This fact was isolated by our team as being thanks to the nature of the model, which cleverly disregards noise and retains information. However, the dropped subsets are then added back into the data, which results in a medium length computation time for the model (although higher than most others).\n",
    "\n",
    "- **NHLs** performed moderately on all aspects previously considered for the other models. The model did not stand out for its accuracy, nor its structural complexity - but the computation time was quicker than others. The convergence time was slower than DNNs but still quicker than most - which is, again, thanks to the properties emerging from the addition of skip/null layers to the architecture. \n",
    "\n",
    "- **AutoEncoders** performed surprisingly well in terms of accuracy for our problem given that they are suited better for imputation rather than classification. We gave them an enhanced initial expectation based on how they predominantly do well on sparse/small datasets - however, they did the worst out of all implemented models in predicting the smaller classes. This can be due to multiple factors such as: architecture, activation functions, iterations etc. Computation time was also lengthy, certainly based on the encoding + decoding layers taking extra time and memory.\n",
    "\n",
    "~ _Naive Bayes_ was used as a model to contrast AEs in particular. It performed excellently and quickly on the given task, beating all the NNs in terms of speed and memory consumption. This was perfectly expected. Performance would've likely dropped on harder inference goals. More importantly, with sufficient learning its accuracy rate would stop increasing/increase linearly as opposed to the NN models.\n",
    "\n",
    "~ _Extreme Learning Machines_ were used to contrast FFNNs and all other models. These gave out a perfect accuracy regardless of architectural choice. We're tempted to believe that there's some post-training done in the background, as the response time was also almost immediate. Their implementation was nonetheless insightful per se.\n",
    "\n",
    "~ _Random Forests_ were used for the same purpose as ELMs and achieved a fantastic computation time. The accuracy attained topped over half of the implemented models, and its appliance was straightforward and well fit for the inference goal. In addition, KNN(K Nearest Neighbours) were considered as a comparison model - but RFs were chosen based on prestige. Both models were likely (which was in fact confirmed) to be better suits for our simple inference goal than most of the more structurally rich Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "* In terms of _computation time_ on the whole data (i.e. on the training partition on the dataset for learning, with targeted testing on the pre-assigned test subset), the best performing models, _apart from those which were not Neural Network types_, were as follows:\n",
    "\n",
    "1) **The SLP** - This model performed quickest entirely thanks to its simple structure, as well as having a slightly simplified inference goal set. This is in line with our prior expectations.\n",
    "\n",
    "2) **NHL** - This model was fairly quick based on the architecture of choice. Adding skip layers often helps the computation speed, and the simplest format is having all of them be null layers. We expected this model to perform quicker than the SLP.\n",
    "\n",
    "3) **DDNN** - This model's architecture helps it reach convergence fast enough for it to require less learning time than others. It exchanges the top places' higher speed for a better overall precision.\n",
    "\n",
    "* In terms of _overall accuracy_, with the same specifications as above, the best performing models were:\n",
    "\n",
    "1) **FFNN** - The R version performed almost perfectly on sampled sets, whereas the Keras (Python) variant peaked in performance above all other implemented types on the whole dataset. It exchanges a slow learning rate - although on one of the simpler structures - for excellent accuracy.\n",
    "\n",
    "2) **NHL** - This model performed indistinctively from the 3rd place, so they're placed in a more or less arbitrary ranking. They share similar architectural properties from our target's perspective, which would explain how they arrive at similar conclusions.\n",
    "\n",
    "3) **DDNN** - The same specifications apply as for the _NHL_. In our limited testing on the HPC, this model performed worse at the order of 10^-4.\n",
    "\n",
    "* In terms of _recall_ with the same specifications, the following models excelled:\n",
    "\n",
    "1) **FFNN** - Both the R and the Keras variants take 1st and 2nd place respectively in this ranking, so we won't discuss them separately. \n",
    "\n",
    "2) **NHL** - Similar remarks as those in the 'overall accuracy' ranking section apply. It, once again, slightly outperformed the Dropout model in this category as well.\n",
    "\n",
    "3) **DDNN** - This model did not achieve a spectacular recall score, though none of the models except FFNN did. However, it performed better than the AE by a margin.\n",
    "\n",
    "\n",
    "In conclusion, our general choice of types and their parameters was highly insightful for creating comparisons. In general, provided the set aim, simpler structures and types were preferred. Our team expected this from the start, especially so given our choice of task was a classification problem which we found unfit for most NN types - as they often deal with image recognition. Another approach was to translate our goal into an imagery problem, but this is beyond the scope of our project now. Feed Forward networks tend to perform best for all general purposes, though auto-encoders presented an interesting analysis as well as the NHL/DNN's.\n",
    "\n",
    "A more thorough investigation into the performance of all models can be found in our 'Performance Analysis' report, namely [here](https://github.com/Galeforse/DST-Assessment-04/blob/main/Report/06%20-%20Performance%20Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Were neural networks the right choice for our goal?**\n",
    "\n",
    "Yes and no. Perhaps the better question would be if our inference goal of choice was suited to neural networks. On one hand, as the accuracy, precision and all other result would rightfully prove, almost all models achieved satisfactory results. However, on the other hand, the simpler models we used for comparison almost unequivocally performed both better in terms of accuracy, but also much quicker, with far less computation demand. All NN models were ran (when it came to testing on the whole dataset) exclusively on the HPC, whereas the others could be run on local CPUs in a matter of minutes. This certainly does not mean that neural networks are generally a bad choice. It is clear that the models we chose to examine learned much more fundamental correlations and distributions between features, parameters and datapoints than more 'trivial' models would. In terms of classification, all types of NNs we considered tackle the more profound concern of 'learning to learn' - in the sense that they would perform much better on out of sample datapoints than other, more basic methods would. Random forests, KNN and Naive Bayes are tools renowned for basic classification. All of them are known to perform extremely well when it comes to dividing one or more features into a limited number of classes. This performance is expected to drop if, unlike our toy example where we only had fewer than 10 distinct types of attacks, the classification would take place in the order of hundreds or thousands of categories. This is not the case for neural networks. Thanks to their intrinsic structure, NNs are up to the task of performing any sort of classification, provided enough material to learn from. We tried to replicate this condition by choosing a massive dataset, and in this sense we succeeded. However, it wouldn't be an unfair remark to say that we 'cut toast with a longsword' by deploying entire neural network models for a single feature classification, as our comparisons with easier models has proven.\n",
    "\n",
    "**Can we say which type is best for classification?**\n",
    "\n",
    "We certainly can't extrapolate such a result from a single example. However, this question is more so answered by the research done prior to starting the project. Given an extensive list of all (most of the) types of neural networks existing, a large portion of them were unsuited for various reasons, including: not being directly applicable in our circumstances, not existing per se in Python/R or only existing via manual implementation (e.g Boltzmann, Markov) or, most often, not being suited for classification a priori. By the last remark we mean, as mentioned above, that we could theoretically transform our problem into an imagery one, but that would require more time than we could afford for the limited scope of this work. We're hence tempted to reply that while we certainly can't know _all_ the types which could be good in the general context of classification, we learned a few that seem to be. In particular, FFNN with or without convolutional layers, only skip layers networks and - subject to the complexity of your classification problem - possibly the Perceptron all provided good results. AutoEncoders are generally expected to perform quite good on this subject in the literature, especially if lacking in density in your dataset, but our experience with them differed.\n",
    "\n",
    "**How could we have improved our results?**\n",
    "\n",
    "In short, it's difficult to tell. Different choices of parameters, activation functions, number of iterations, structures, types and others are the main ideas that come to mind when trying to modify a network to enhance its learning adequacy and power. The manner of modifying these is, however, a very much active research subject in the area. From the limited experience of this project we could tell that, given the easy inference goal, easier models presented the tendency of outperforming complex ones. Autoencoders, scaling amounts of layers and early attempts at constructing more complex graph motives such as Turan or Chun-Lu star-like graphs all ended up in either overfitting by a large margin, or drastically reducing the accuracy. This, combined with the other results our team obtained, shows that more complicated architectures and models are positively correlated with tackling non-trivial goals; and simpler techniques not only suffice, but perform better on trivial goals. One sure but tedious way to thus potentially improve our results would be testing more combinations of those choices presented above and testing their performances one by one. We could rely on the assumption we've drawn so far that a simpler model is preferred to our end, and consequently decide which route we aim to implement our 'simplification'. One such way was the SLP, which did not awe in terms of accuracy. On the other end, NHL managed to perform better, but was still behind classical FFNNs. This last remark would indicate that we're looking for a simple architecture such as those in FFNN, with 2 or 3 hidden layers, on some consistent number of vertices. What then remains to be played with is, quite importantly, algorithms, but we could also experiment with how we're deploying the structure of the NN. All these trial and error possibilities represent a naive approach to trying to better our results. In a larger project, perhaps, a more sophisticated method is preferable to be sought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors\n",
    "\n",
    "_Link to the contributor's Git profile_  ~  _Link to their work on this project_\n",
    "\n",
    "[Alex Caian](https://github.com/Alex-Caian) ~ [Work #1](https://github.com/Galeforse/DST-Assessment-04/tree/main/Alex%20Caian)\n",
    "\n",
    "[Gabriel Grant](https://github.com/Galeforse) ~ [Work #2](https://github.com/Galeforse/DST-Assessment-04/tree/main/Gabriel%20Grant)\n",
    "\n",
    "[Luke Hawley](https://github.com/LukeHGithub) ~ [Work #3](https://github.com/Galeforse/DST-Assessment-04/tree/main/Luke%20Hawley)\n",
    "\n",
    "[Matt Corrie](https://github.com/mc17336) ~ [Work #4](https://github.com/Galeforse/DST-Assessment-04/tree/main/Matt%20Corrie)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
