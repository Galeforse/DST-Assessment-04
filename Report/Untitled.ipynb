{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final report will take a similar structure to the previous reports compiled. We will report the accuracy of our models using classification matrices for each method applied to the data set to compare and contrast what was achieved. This will allow us to create a summary graph comparing the neural networks to the Naive Bayes and Random Forest methods employed as comparisons. We will then use bar graphs to directly compare the accuracies and draw conclusions on the advantages and draw backs of each method. Further results, visualisations and analysis of each model can be found in each indivduals folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcation Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def classification_eval(y_true,y_pred):\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    C = confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    print('Classification report')\n",
    "    print(classification_report(y_true, y_pred, target_names = (['Normal','Exploits','Reconnaissance','DoS','Generic',\n",
    "                                                                 'Shellcode','Fuzzers','Worms','Backdoor','Analysis' ]), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matt's NN Predictions\n",
    "Matt_NN_Actual = pickle.load(open('../Matt Corrie/Y_test.p','rb'))\n",
    "Matt_NN_Pred = pickle.load(open('../Matt Corrie/Y_predictions.p','rb'))\n",
    "\n",
    "# Matt's RF Predictions\n",
    "Matt_RF_Actual = pickle.load(open('../Matt Corrie/Y_test.p','rb'))\n",
    "Matt_RF_Pred = pickle.load(open('../Matt Corrie/rf_predictions.p','rb'))\n",
    "\n",
    "# Alex's NN Predictions\n",
    "\n",
    "# Luke's NB Predictions\n",
    "\n",
    "# Luke's AE Predictions\n",
    "\n",
    "# Gab's Dropout NN Predictions\n",
    "\n",
    "# Gab's No Hidden Layer NN Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the report for the Neural Network model prepared by Matt.\n",
      "-------------------------------------------------------\n",
      "Confusion Matrix\n",
      "Classification report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal     0.9961    0.9963    0.9962    221772\n",
      "      Exploits     0.5966    0.8844    0.7125      4411\n",
      "Reconnaissance     0.8656    0.7945    0.8285      1411\n",
      "           DoS     0.4400    0.0334    0.0621      1647\n",
      "       Generic     0.9943    0.9857    0.9900     21642\n",
      "     Shellcode     0.6531    0.8000    0.7191       160\n",
      "       Fuzzers     0.5853    0.6139    0.5992      2437\n",
      "         Worms     0.0000    0.0000    0.0000         6\n",
      "      Backdoor     1.0000    0.0480    0.0916       250\n",
      "      Analysis     0.3750    0.0112    0.0217       269\n",
      "\n",
      "      accuracy                         0.9803    254005\n",
      "     macro avg     0.6506    0.5167    0.5021    254005\n",
      "  weighted avg     0.9799    0.9803    0.9778    254005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Below is the report for the Neural Network model prepared by Matt.')\n",
    "print('-------------------------------------------------------')\n",
    "classification_eval(Matt_NN_Actual,Matt_NN_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the report for the Random Forest model prepared by Matt.\n",
      "-------------------------------------------------------\n",
      "Confusion Matrix\n",
      "Classification report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal     0.9977    0.9985    0.9981    221772\n",
      "      Exploits     0.6309    0.8216    0.7137      4411\n",
      "Reconnaissance     0.9222    0.7725    0.8407      1411\n",
      "           DoS     0.3059    0.2325    0.2642      1647\n",
      "       Generic     0.9977    0.9876    0.9926     21642\n",
      "     Shellcode     0.7644    0.8313    0.7964       160\n",
      "       Fuzzers     0.7721    0.7091    0.7393      2437\n",
      "         Worms     0.0000    0.0000    0.0000         6\n",
      "      Backdoor     0.7917    0.0760    0.1387       250\n",
      "      Analysis     0.7742    0.0892    0.1600       269\n",
      "\n",
      "      accuracy                         0.9835    254005\n",
      "     macro avg     0.6957    0.5518    0.5644    254005\n",
      "  weighted avg     0.9837    0.9835    0.9827    254005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Below is the report for the Random Forest model prepared by Matt.')\n",
    "print('-------------------------------------------------------')\n",
    "classification_eval(Matt_RF_Actual,Matt_RF_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def cal_APRF(pre,test):\n",
    "    result = []\n",
    "    ACC = round(100*(accuracy_score(pre, test)),3)\n",
    "    result.append(ACC)\n",
    "    # Precision\n",
    "    P = round(100*(precision_score(pre, test, average=\"weighted\")),3)\n",
    "    result.append(P)\n",
    "    # Recall \n",
    "    R = round(100*(recall_score(pre, test, average=\"weighted\")),3)\n",
    "    result.append(R)\n",
    "    # F1-Score\n",
    "    F = round(100*(f1_score(pre, test, average=\"weighted\")),3)\n",
    "    result.append(F)\n",
    "    return result\n",
    "\n",
    "score1 = cal_APRF(Matt_NN_Actual,Matt_NN_Pred)\n",
    "score2 = cal_APRF(Matt_RF_Actual,Matt_RF_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_APRF(score1,score2,score3,score4,score5,score6,score7):\n",
    "    plt.figure(figsize=(12,7))\n",
    "    metric = ('Accuracy', 'Precison', 'Recall', 'F1_score', )\n",
    "    bar_width = 0.2  \n",
    "    index1 = np.arange(len(metric))\n",
    "    index2 = index1 + bar_width  \n",
    "    index3 = index2 + bar_width  \n",
    "\n",
    "    p1 = plt.bar(index1, height=score1, width=bar_width, color='deepskyblue',label='Keras Feed Forward Neural Network')\n",
    "    p2 = plt.bar(index2, height=score2, width=bar_width, color='sandybrown',label='Random Forest')\n",
    "    p3 = plt.bar(index3, height=score3, width=bar_width, color='limegreen',label='R Feed Forward Neural Network')\n",
    "    p4 = plt.bar(index3, height=score3, width=bar_width, color='maroon',label='Naive Bayes')\n",
    "    p5 = plt.bar(index3, height=score3, width=bar_width, color='black',label='Auto Encoder')\n",
    "    p6 = plt.bar(index3, height=score3, width=bar_width, color='purple',label='Dropout Neural Network')\n",
    "    p7 = plt.bar(index3, height=score3, width=bar_width, color='gold',label='Dropout Neural Network')\n",
    "    \n",
    "    \n",
    "    #Mark the value on the graph\n",
    "    for p in p1:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p2:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p3:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p4:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p5:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p6:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p7:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))  \n",
    "    plt.xticks(index1 + bar_width/2, metric, fontsize=16)  \n",
    "    plt.ylabel('Metric_Value', fontsize=16)  \n",
    "    plt.title('Evaluation', fontsize=24, fontweight= 'black')  \n",
    "    plt.show()\n",
    "    \n",
    "draw_APRF(score1,score2,score3,score4,score5,score6,score7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
