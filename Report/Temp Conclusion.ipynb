{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Our team agreed on the task of classifying types of attacks on a toy dataset which can be found extracted [here](https://github.com/Galeforse/DST-Assessment-04/tree/main/Data) by means of neural networks modelling. Every contributor was free to use any and all variations of NNs on the same inference goal, as well as using other models in parallel for comparison. Our work was performed across 2 main languages: Python and R; as well as the computationally/memory demanding models being run on the University's HPC (High Performance Computer). Each contributor chose to test out at least two main types of models:\n",
    "\n",
    "[Alex](https://github.com/Galeforse/DST-Assessment-04/tree/main/Alex%20Caian) chose to test a _Feed Forward Nueral Network_ (FFNN) and a _Single Layer Perceptron_ (SLP)\n",
    "\n",
    "[Gabriel](https://github.com/Galeforse/DST-Assessment-04/tree/main/Gabriel%20Grant) chose to test a _Dropout Neural Network_ and a _No Hidden Layers_ (NHL)/Only Skip Layers NN\n",
    "\n",
    "[Luke](https://github.com/Galeforse/DST-Assessment-04/tree/main/Luke%20Hawley) chose to test an _AutoEncoder Nueral Network_ (AE) against a _Naive Bayes_ on the same inference goal and\n",
    "\n",
    "[Matt](https://github.com/Galeforse/DST-Assessment-04/tree/main/Matt%20Corrie) chose to test a _Feed Forward Neural Network_ (FFNN) against an _Extreme Learning Machine_ (ELM) on the same inference goal, as well as _Random Forests_ (RF).\n",
    "\n",
    "The main takeaways were as follows:\n",
    "\n",
    "- **FFNNs** tend to perform really well on the given inference goal, especially so on simpler (particularly uniform) architectures, regardless of the activation function/algorithm. Our team narrowed this aspect down mostly to the fact that the set goal is easily attainable for models in general, as comparisons will tell, so NNs did **not** require much learning. Computation was expectedly long, with even further delays and impediments from the HPC used, and results were both accurate and consistent among themselves.\n",
    "\n",
    "- **The SLP** was only tested on a binary application of attack v.s no attack. Its performance dropped significantly on an increased amount of parameters, suggesting that some features added proportionally more noise than signal to the set target. The accuracy peaked on a 'medium' complexity (i.e. amount of nodes within layer), where it performed slightly worse than all other models. Computation was extremely quick, however - this being its greatest advantage as a method.\n",
    "\n",
    "- **Dropout NNs** achieved high accuracy, comparable to that of FFNNs and NHLs. Uniquely, DNNs presented a stable learning rate all across the training phase and reached convergence quicker than any other implemented model. This fact was isolated by our team as being thanks to the nature of the model, which cleverly disregards noise and retains information. However, the dropped subsets are then added back into the data, which results in a medium length computation time for the model (although higher than most others).\n",
    "\n",
    "- **NHLs** performed moderately on all aspects previously considered for the other models. The model did not stand out for its accuracy, nor its structural complexity - but the computation time was quicker than others. The convergence time was slower than DNNs but still quicker than most - which is, again, thanks to the properties emerging from the addition of skip/null layers to the architecture. \n",
    "\n",
    "- **AutoEncoders** performed unexpectedly poor in terms of accuracy. We gave them an enhanced initial expectation based on how they predominantly do well on sparse/small datasets - however, they did the worst out of all implemented models in predicting. This can be due to multiple factors such as: architecture, activation functions, iterations etc. Notably, upon reconsideration of those the results improved remarkably, yet not to the standard set by our other NNs. Computation time was also lengthy, certainly based on the encoding+decoding layers taking extra time and memory.\n",
    "\n",
    "~ _Naive Bayes_ was used as a model to contrast AEs in particular. It performed excellently and quickly on the given task, beating all the NNs in terms of speed and memory consumption. This was perfectly expected. Performance would've likely dropped on harder inference goals. More importantly, with sufficient learning its accuracy rate would stop increasing/increase linearly as opposed to the NN models.\n",
    "\n",
    "~ _Extreme Learning Machines_ were used to contrast FFNNs and all other models. These gave out a perfect accuracy regardless of architectural choice. We're tempted to believe that there's some post-training done in the background, as the response time was also almost immediate. Their implementation was nonetheless insightful per se.\n",
    "\n",
    "~ _Random Forests_ were used for the same purpose as ELMs and achieved a fantastic computation time. The accuracy attained topped over half of the implemented models, and its appliance was straightforward and well fit for the inference goal. In addition, KNN(K Nearest Neighbours) were considered as a comparison model - but RFs were chosen based on prestige. Both models were likely (which was in fact confirmed) to be better suits for our simple inference goal than most of the more structurally rich Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "* In terms of _computation time_ on the whole data (i.e. on the training partition on the dataset for learning, with targeted testing on the pre-assigned test subset), the best performing models, _apart from those which were not Neural Network types_, were as follows:\n",
    "\n",
    "1) **The SLP** - This model performed quickest entirely thanks to its simple structure, as well as having a slightly simplified inference goal set. This is in line with our prior expectations.\n",
    "\n",
    "2) **NHL** - This model was fairly quick based on the architecture of choice. Adding skip layers often helps the computation speed, and the simplest format is having all of them be null layers. We expected this model to perform quicker than the SLP.\n",
    "\n",
    "3) **DDNN** - This model's architecture helps it reach convergence fast enough for it to require less learning time than others. It exchanges the top places' higher speed for a better overall precision.\n",
    "\n",
    "* In terms of _overall accuracy_, with the same specifications as above, the best performing models were:\n",
    "\n",
    "1) **FFNN** - The R version performed almost perfectly on sampled sets, whereas the Keras (Python) variant peaked in performance above all other implemented types on the whole dataset. It exchanges a slow learning rate - although on one of the simpler structures - for excellent accuracy.\n",
    "\n",
    "2) **NHL** - This model performed indistinctively from the 3rd place, so they're placed in a more or less arbitrary ranking. They share similar architectural properties from our target's perspective, which would explain how they arrive at similar conclusions.\n",
    "\n",
    "3) **DDNN** - The same specifications apply as for the _NHL_. In our limited testing on the HPC, this model performed worse at the order of 10^-4.\n",
    "\n",
    "* In terms of _recall_ with the same specifications, the following models excelled:\n",
    "\n",
    "1) **FFNN** - Both the R and the Keras variants take 1st and 2nd place respectively in this ranking, so we won't discuss them separately. \n",
    "\n",
    "2) **NHL** - Similar remarks as those in the 'overall accuracy' ranking section apply. It, once again, slightly outperformed the Dropout model in this category as well.\n",
    "\n",
    "3) **DDNN** - This model did not achieve a spectacular recall score, though none of the models except FFNN did. However, it performed better than the AE by a margin.\n",
    "\n",
    "\n",
    "In conclusion, our general choice of types and their parameters was highly insightful for creating comparisons. In general, provided the set aim, simpler structures and types were preferred. Our team expected this from the start, especially so given our choice of task was a classification problem which we found unfit for most NN types - as they often deal with image recognition. Another approach was to translate our goal into an imagery problem, but this is beyond the scope of our project now. Feed Forward networks tend to perform best for all general purposes, though auto-encoders presented an interesting analysis as well as the NHL/DNN's.\n",
    "\n",
    "A more thorough investigation into the performance of all models can be found in our 'Performance Analysis' report, namely [here](https://github.com/Galeforse/DST-Assessment-04/blob/main/Report/06%20-%20Performance%20Analysis.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Reflections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
