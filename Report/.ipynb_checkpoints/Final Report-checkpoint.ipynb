{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final report will take a similar structure to the previous reports compiled. We will report the accuracy of our models using classification matrices for each method applied to the data set to compare and contrast what was achieved. This will allow us to create a summary graph comparing the neural networks to the Naive Bayes and Random Forest methods employed as comparisons. We will then use bar graphs to directly compare the accuracies and draw conclusions on the advantages and draw backs of each method. Further results, visualisations and analysis of each model can be found in each indivduals folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifcation Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def classification_eval(y_true,y_pred):\n",
    "    \n",
    "    print(\"Confusion Matrix\")\n",
    "    C = confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    print('Classification report')\n",
    "    print(classification_report(y_true, y_pred, target_names = (['Normal','Exploits','Reconnaissance','DoS','Generic',\n",
    "                                                                 'Shellcode','Fuzzers','Worms','Backdoor','Analysis' ]), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matt's NN Predictions\n",
    "Matt_NN_Actual = pickle.load(open('../Matt Corrie/Y_test.p','rb'))\n",
    "Matt_NN_Pred = pickle.load(open('../Matt Corrie/Y_predictions.p','rb'))\n",
    "\n",
    "# Matt's RF Predictions\n",
    "Matt_RF_Actual = pickle.load(open('../Matt Corrie/Y_test.p','rb'))\n",
    "Matt_RF_Pred = pickle.load(open('../Matt Corrie/rf_predictions.p','rb'))\n",
    "\n",
    "# Alex's NN Predictions\n",
    "Alex_NN_Pred = pd.read_csv('https://github.com/Galeforse/DST-Assessment-04/raw/main/Alex%20Caian/tab1_2.csv')\n",
    "\n",
    "# Luke's NB Predictions\n",
    "Luke_NB_Actual = pickle.load(open('../Luke Hawley/Y_test_Luke.p','rb'))\n",
    "Luke_NB_Pred = pickle.load(open('../Luke Hawley/Y_predictions_Luke.p','rb'))\n",
    "\n",
    "# Gab's Dropout NN Predictions\n",
    "Gab_D_Actual = pickle.load(open('../Gabriel Grant/HPC/Y_test_c_drop.pkl','rb'))\n",
    "Gab_D_Pred = pickle.load(open('../Gabriel Grant/HPC/Y_predictions_drop.pkl','rb'))\n",
    "\n",
    "# Gab's No Hidden Layer NN Predictions\n",
    "Gab_NH_Actual = pickle.load(open('../Gabriel Grant/HPC/Y_test_c2.pkl','rb'))\n",
    "Gab_NH_Pred = pickle.load(open('../Gabriel Grant/HPC/Y_predictions.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the report for the Neural Network model prepared by Matt.\n",
      "------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "Classification report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal     0.9961    0.9963    0.9962    221772\n",
      "      Exploits     0.5966    0.8844    0.7125      4411\n",
      "Reconnaissance     0.8656    0.7945    0.8285      1411\n",
      "           DoS     0.4400    0.0334    0.0621      1647\n",
      "       Generic     0.9943    0.9857    0.9900     21642\n",
      "     Shellcode     0.6531    0.8000    0.7191       160\n",
      "       Fuzzers     0.5853    0.6139    0.5992      2437\n",
      "         Worms     0.0000    0.0000    0.0000         6\n",
      "      Backdoor     1.0000    0.0480    0.0916       250\n",
      "      Analysis     0.3750    0.0112    0.0217       269\n",
      "\n",
      "      accuracy                         0.9803    254005\n",
      "     macro avg     0.6506    0.5167    0.5021    254005\n",
      "  weighted avg     0.9799    0.9803    0.9778    254005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Below is the report for the Neural Network model prepared by Matt.')\n",
    "print('------------------------------------------------------------------')\n",
    "classification_eval(Matt_NN_Actual,Matt_NN_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the binary confusion matrix for the Neural Network model prepared by Alex.\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64068</td>\n",
       "      <td>46587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87</td>\n",
       "      <td>397268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1\n",
       "0  64068   46587\n",
       "1     87  397268"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Below is the binary confusion matrix for the Neural Network model prepared by Alex.')\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "Alex_NN_Pred.columns = ['na','0','1']\n",
    "Alex_NN_Pred.drop('na', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the report for the Random Forest model prepared by Matt.\n",
      "-----------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "Classification report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal     0.9977    0.9985    0.9981    221772\n",
      "      Exploits     0.6309    0.8216    0.7137      4411\n",
      "Reconnaissance     0.9222    0.7725    0.8407      1411\n",
      "           DoS     0.3059    0.2325    0.2642      1647\n",
      "       Generic     0.9977    0.9876    0.9926     21642\n",
      "     Shellcode     0.7644    0.8313    0.7964       160\n",
      "       Fuzzers     0.7721    0.7091    0.7393      2437\n",
      "         Worms     0.0000    0.0000    0.0000         6\n",
      "      Backdoor     0.7917    0.0760    0.1387       250\n",
      "      Analysis     0.7742    0.0892    0.1600       269\n",
      "\n",
      "      accuracy                         0.9835    254005\n",
      "     macro avg     0.6957    0.5518    0.5644    254005\n",
      "  weighted avg     0.9837    0.9835    0.9827    254005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Below is the report for the Random Forest model prepared by Matt.')\n",
    "print('-----------------------------------------------------------------')\n",
    "classification_eval(Matt_RF_Actual,Matt_RF_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Below is the report for the Naive Bayes model prepared by Luke.')\n",
    "print('------------------------------------------------------------------')\n",
    "classification_eval(Luke_NB_Actual,Luke_NB_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the report for the Dropout Neural Network model prepared by Gabriel.\n",
      "-----------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "Classification report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal     0.9938    0.9972    0.9955    221772\n",
      "      Exploits     0.5618    0.9231    0.6985      4411\n",
      "Reconnaissance     0.8823    0.7172    0.7912      1411\n",
      "           DoS     0.6667    0.0109    0.0215      1647\n",
      "       Generic     0.9965    0.9778    0.9871     21642\n",
      "     Shellcode     0.7500    0.6000    0.6667       160\n",
      "       Fuzzers     0.5628    0.3878    0.4592      2437\n",
      "         Worms     0.0000    0.0000    0.0000         6\n",
      "      Backdoor     0.8333    0.0200    0.0391       250\n",
      "      Analysis     0.0000    0.0000    0.0000       269\n",
      "\n",
      "      accuracy                         0.9782    254005\n",
      "     macro avg     0.6247    0.4634    0.4659    254005\n",
      "  weighted avg     0.9783    0.9782    0.9748    254005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corri\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Below is the report for the Dropout Neural Network model prepared by Gabriel.')\n",
    "print('-----------------------------------------------------------------------------')\n",
    "classification_eval(Gab_D_Actual,Gab_D_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the report for the Neural Network model without hidden layers prepared by Gabriel.\n",
      "-------------------------------------------------------------------------------------------\n",
      "Confusion Matrix\n",
      "Classification report\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Normal     0.9951    0.9964    0.9958    221772\n",
      "      Exploits     0.6074    0.8603    0.7121      4411\n",
      "Reconnaissance     0.8893    0.7633    0.8215      1411\n",
      "           DoS     0.3639    0.1324    0.1941      1647\n",
      "       Generic     0.9959    0.9828    0.9893     21642\n",
      "     Shellcode     0.7019    0.7063    0.7040       160\n",
      "       Fuzzers     0.5636    0.5363    0.5496      2437\n",
      "         Worms     0.0000    0.0000    0.0000         6\n",
      "      Backdoor     0.4706    0.0320    0.0599       250\n",
      "      Analysis     0.3333    0.0260    0.0483       269\n",
      "\n",
      "      accuracy                         0.9794    254005\n",
      "     macro avg     0.5921    0.5036    0.5075    254005\n",
      "  weighted avg     0.9782    0.9794    0.9777    254005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Below is the report for the Neural Network model without hidden layers prepared by Gabriel.')\n",
    "print('-------------------------------------------------------------------------------------------')\n",
    "classification_eval(Gab_NH_Actual,Gab_NH_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corri\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def cal_APRF(pre,test):\n",
    "    result = []\n",
    "    ACC = round(100*(accuracy_score(pre, test)),3)\n",
    "    result.append(ACC)\n",
    "    # Precision\n",
    "    P = round(100*(precision_score(pre, test, average=\"weighted\")),3)\n",
    "    result.append(P)\n",
    "    # Recall \n",
    "    R = round(100*(recall_score(pre, test, average=\"weighted\")),3)\n",
    "    result.append(R)\n",
    "    # F1-Score\n",
    "    F = round(100*(f1_score(pre, test, average=\"weighted\")),3)\n",
    "    result.append(F)\n",
    "    return result\n",
    "\n",
    "score1 = cal_APRF(Matt_NN_Actual,Matt_NN_Pred)\n",
    "score2 = cal_APRF(Matt_RF_Actual,Matt_RF_Pred)\n",
    "score3 = cal_APRF(Luke_NB_Actual,Luke_NB_Pred)\n",
    "score4 = cal_APRF(Gab_D_Actual,Gab_D_Pred)\n",
    "score5 = cal_APRF(Gab_NH_Actual,Gab_NH_Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we compute Alex's scores seperately since we dont get a list of predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_APRF(score1,score2,score3,score4,score5,score6):\n",
    "    plt.figure(figsize=(12,7))\n",
    "    metric = ('Accuracy', 'Precison', 'Recall', 'F1_score', )\n",
    "    bar_width = 0.2  \n",
    "    index1 = np.arange(len(metric))\n",
    "    index2 = index1 + bar_width  \n",
    "    index3 = index2 + bar_width\n",
    "    index4 = index3 + bar_width\n",
    "    index5 = index4 + bar_width\n",
    "    index6 = index5 + bar_width\n",
    "\n",
    "    p1 = plt.bar(index1, height=score1, width=bar_width, color='deepskyblue',label='Keras Feed Forward Neural Network')\n",
    "    p2 = plt.bar(index2, height=score2, width=bar_width, color='sandybrown',label='Random Forest')\n",
    "    p3 = plt.bar(index3, height=score3, width=bar_width, color='maroon',label='Naive Bayes')\n",
    "    p4 = plt.bar(index4, height=score4, width=bar_width, color='purple',label='Dropout Neural Network')\n",
    "    p5 = plt.bar(index5, height=score5, width=bar_width, color='black',label='Neural Network without Hidden Layers')\n",
    "    p6 = plt.bar(index6, height=score6, width=bar_width, color='limegreen',label='R Feed Forward Neural Network')\n",
    "\n",
    "    \n",
    "    \n",
    "    #Mark the value on the graph\n",
    "    for p in p1:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p2:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p3:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p4:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p5:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p6:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "    for p in p7:\n",
    "        height = p.get_height()\n",
    "        plt.text(p.get_x() + p.get_width() / 2, height+1, str(height), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))  \n",
    "    plt.xticks(index1 + bar_width/2, metric, fontsize=16)  \n",
    "    plt.ylabel('Metric_Value', fontsize=16)  \n",
    "    plt.title('Evaluation', fontsize=24, fontweight= 'black')  \n",
    "    plt.show()\n",
    "    \n",
    "draw_APRF(score1,score2,score3,score4,score5,score6,score7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
